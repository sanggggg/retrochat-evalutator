# Retrochat Evaluator Configuration Example
# Copy this file to config.yaml and modify as needed

# LLM Configuration for Rubric Extraction (training step 1)
extraction_llm:
  model_name: gemini-2.5-pro-preview-06-05
  temperature: 0.3
  max_tokens: 4096
  max_retries: 3
  retry_delay: 1.0

# LLM Configuration for Rubric Summarization (training step 2)
summarization_llm:
  model_name: gemini-2.5-pro-preview-06-05
  temperature: 0.3
  max_tokens: 8192
  max_retries: 3
  retry_delay: 1.0

# LLM Configuration for Evaluation/Scoring (validate/evaluate)
evaluation_llm:
  model_name: gemini-2.5-pro-preview-06-05
  temperature: 0.1
  max_tokens: 1024
  max_retries: 3
  retry_delay: 1.0

# Shared rate limiter configuration
rate_limiter:
  requests_per_second: 1.0
  check_every_n_seconds: 0.1
  max_bucket_size: 10

# Training Configuration
training:
  score_top_percentile: 10.0
  score_name: default
  max_concurrent_extractions: 5

  # Rubric extraction configuration
  extraction_min_rubrics: 3
  extraction_max_rubrics: 7

  # Summarization method: "llm" or "semantic_clustering"
  summarization_method: llm

  # Summarization configuration (used for both LLM and semantic_clustering methods)
  min_rubrics: 5
  max_rubrics: 10

  # Semantic clustering options (used when summarization_method is semantic_clustering)
  embedding_model: models/text-embedding-004
  umap_n_neighbors: 15
  umap_n_components: 5
  umap_metric: cosine
  min_cluster_size: 2

# Evaluation Configuration
evaluation:
  max_concurrent: 10
  timeout_per_rubric: 60
  retry_on_parse_failure: true
  score_scale: [1, 5]

# Path Configuration
prompts_dir: prompts
dataset_dir: ./raw-data
dataset_manifest: ./dataset.json
rubrics_path: ./rubrics.json
output_path: ./output.json

# Logging
log_level: INFO
