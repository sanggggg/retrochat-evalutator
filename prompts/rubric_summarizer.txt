You are an expert at synthesizing evaluation criteria for AI agent interactions.

You have been given rubrics extracted from multiple high-quality chat sessions. Your task is to consolidate these into a final, coherent set of evaluation rubrics that assess only the human user (messages labeled `role: user`) and never the AI assistant.

## Extracted Rubrics from All Sessions
{all_rubrics}

## Task
Create a final list of {summarization_min_rubrics}-{summarization_max_rubrics} evaluation rubrics by:
1. Identifying common themes across the extracted rubrics
2. Merging similar or overlapping rubrics
3. Removing redundant or overly specific rubrics
4. Ensuring comprehensive coverage of user efficiency aspects
5. Making criteria clear and consistently scorable

## Output Format
Provide your response as a JSON object with the final rubrics:

```json
{{
  "rubrics": [
    {{
      "id": "rubric_001",
      "name": "Clear Initial Requirements",
      "description": "The user provides complete, unambiguous requirements in their initial request",
      "scoring_criteria": "1: Vague request requiring multiple clarifications\n2: Partial requirements, missing key details\n3: Adequate requirements with minor gaps\n4: Clear requirements with most details\n5: Comprehensive requirements covering edge cases",
      "weight": 1.0
    }}
  ],
  "consolidation_notes": "Brief explanation of how rubrics were merged/prioritized"
}}
```

Ensure each rubric:
- Has a unique, descriptive name
- Has clear, objective scoring criteria
- Is applicable across different types of coding tasks
- Focuses exclusively on the human USER (messages with `role: user`), not AI performance
